{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Up2Iso/label_selection_bias/blob/main/label_selection_bias.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcCtC8Ba0SoV",
        "outputId": "7938634e-d3cf-4a7b-a811-994ee30d14de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ba81WtN8Lez"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install aif360\n",
        "!pip install pgmpy\n",
        "!pip install fairlearn\n",
        "!pip install BlackBoxAuditing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-xqJqjgAsQW",
        "outputId": "3c24c678-320b-4eb8-dadb-30a627a2d110"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:No module named 'tempeh': LawSchoolGPADataset will be unavailable. To install, run:\n",
            "pip install 'aif360[LawSchoolGPA]'\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "# Load all necessary packages\n",
        "\n",
        "from pgmpy.models import BayesianNetwork\n",
        "from pgmpy.factors.discrete import TabularCPD\n",
        "from pgmpy.sampling import BayesianModelSampling\n",
        "from aif360.datasets import BinaryLabelDataset\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from aif360.algorithms.preprocessing import Reweighing\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from aif360.algorithms.preprocessing.reweighing import Reweighing\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "from aif360.datasets import BinaryLabelDataset\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "from aif360.algorithms.inprocessing import AdversarialDebiasing\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_eager_execution()\n",
        "\n",
        "# inizialize random\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LZffCE5DcXH"
      },
      "outputs": [],
      "source": [
        "# Universal Variables\n",
        "n_databases = 10\n",
        "discriminations = [0.1*ind for ind in range(10)]\n",
        "disparity = 0\n",
        "var_n = 20\n",
        "size_train = 10000\n",
        "size_test = 10000\n",
        "threshold = 1\n",
        "doubt = 0\n",
        "label_names = 'Y'\n",
        "favorable_classes = 1\n",
        "sensitive_attribute = 'A'\n",
        "privileged_groups = [{sensitive_attribute: 1}]\n",
        "unprivileged_groups = [{sensitive_attribute: 0}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNZvSK2VJ4hD"
      },
      "outputs": [],
      "source": [
        "def eval_model(predictions, true_labels, sensitive_attribute):\n",
        "    if len(predictions)!=len(true_labels) or len(predictions)!=len(sensitive_attribute):\n",
        "        print('error: length of inputs must be equal')\n",
        "        return False\n",
        "    predictions = pd.Series(predictions)\n",
        "    true_labels = pd.Series(true_labels)\n",
        "    sensitive_attribute = pd.Series(sensitive_attribute)\n",
        "\n",
        "    filtro = (sensitive_attribute == 1)\n",
        "    a1 = predictions[filtro].sum()/(filtro.sum())\n",
        "\n",
        "    filtro = (sensitive_attribute == 0)\n",
        "    a0 = predictions[filtro].sum()/(filtro.sum())\n",
        "\n",
        "    disparity = a1-a0\n",
        "\n",
        "    accuracy = (predictions == true_labels).sum()/len(predictions)\n",
        "\n",
        "    return [accuracy, disparity]\n",
        "\n",
        "def calc_prob(dataframe, y, y_value, X=[], X_value=[]):\n",
        "        '''calculates the conditional probability P(y=y_value| X=X_value) on the dataframe considered'''\n",
        "        filtro = pd.Series([True for ind in range(dataframe.shape[0])])\n",
        "        for ind in range(len(X)):\n",
        "            filtro = filtro & dataframe[X[ind]] == X_value[ind]\n",
        "        den = filtro.sum()\n",
        "        filtro = filtro & dataframe[y] == y_value\n",
        "        num = filtro.sum()\n",
        "        return num/den if den != 0  else 1\n",
        "\n",
        "def prob_G_given_A(disparity):\n",
        "    '''returns [[P(G=0|A=0),P(G=0|A=1)],\n",
        "                [P(G=1|A=0),P(G=1|A=1)] '''\n",
        "    prob= (disparity+1)/2\n",
        "    return [[prob, 1- prob],[1-prob, prob]]\n",
        "\n",
        "def prob_keep(disparity, discrimination):\n",
        "    '''returns [[P(not keep|A=0,G=0),P(not keep=0|A=0,G=1),P(not keep=0|A=1,G=0),P(not keep=0|A=1,G=1)],\n",
        "                [P(keep|A=0,G=0),P(keep|A=0,G=1),P(keep|A=1,G=0),P(keep|A=1,G=1)] '''\n",
        "\n",
        "    if disparity == 1:\n",
        "        odds = [float('inf'), 0]\n",
        "    elif disparity == -1:\n",
        "        odds = [0, float('inf')]\n",
        "    else:\n",
        "        proba = prob_G_given_A(disparity)\n",
        "        odds = [proba[0][0]/proba[1][0], proba[0][1]/proba[1][1]]\n",
        "\n",
        "    #assuming P(keep|y=0,a)*P(keep|y=0,not a)/P(keep|y=1,a)P(keep|y=1,neg a) = 1\n",
        "    q = disparity+discrimination\n",
        "    a = (q-1)*odds[0]\n",
        "    b = (odds[0]*odds[1]+1)*q\n",
        "    c = odds[1]*(q+1)\n",
        "    odd_keep = -b/(2*a) - ((b**2 - 4*a*c)**(0.5))/(2*a)\n",
        "    odd_keep = [odd_keep, 1/odd_keep]\n",
        "    #assuming P(keep|y=0,a=0) = 1 = P(keep|y=1,a=1)\n",
        "    proba = [1, 1/odd_keep[0], odd_keep[1],1]\n",
        "    proba = [[1-ind for ind in proba], proba]\n",
        "    return proba\n",
        "\n",
        "def prob_X_given_AG(threshold):\n",
        "    '''returns [[P(X=0|A=0,G=0),P(X=0|A=0,G=1),P(X=0|A=1,G=0),P(X=0|A=1,G=1)],\n",
        "                [P(X=1|A=0,G=0),P(X=1|A=0,G=1),P(X=1|A=1,G=0),P(X=1|A=1,G=1)] '''\n",
        "    num_points = 4\n",
        "    prob = [0.3, 0.4, 0.5, 0.6]\n",
        "    random.shuffle(prob)\n",
        "    return [prob, [1-prob[i] for i in range(num_points)]]\n",
        "\n",
        "def prob_Y_given_AG(discrimination, disparity):\n",
        "    \"\"\" P(Y=1| A=0,G=0) P(Y=1| A=0,G=1) P(Y=1| A=1,G=0) P(Y=1| A=1,G=1) \"\"\"\n",
        "    prob = [0,0,0,0]\n",
        "    error = doubt\n",
        "    prob[0] = error\n",
        "    prob[3] = 1 - error\n",
        "    #assuming P(y|a,not g) =  P(not y|not a, g)\n",
        "    prob[2] =  (discrimination + error*(1 + disparity))/(1-disparity)\n",
        "    prob[1] = 1-prob[2]\n",
        "    return [[1-ind for ind in prob], prob]\n",
        "\n",
        "def generate_model(threshold, disparity):\n",
        "    cpd_X = [TabularCPD(variable=('X'+str(i)),\n",
        "                       variable_card=2,\n",
        "                       values=prob_X_given_AG(threshold),\n",
        "                       evidence=[sensitive_attribute,'G'],\n",
        "                       evidence_card=[2,2]) for i in range(var_n) ]\n",
        "    cpd_Y = TabularCPD(variable=label_names,\n",
        "                       variable_card=2,\n",
        "                       values=prob_Y_given_AG(discrimination, disparity),\n",
        "                       evidence=[sensitive_attribute,'fair_Y'],\n",
        "                       evidence_card=[2,2])\n",
        "    cpd_A = TabularCPD(variable=sensitive_attribute,\n",
        "                       variable_card=2,\n",
        "                       values=[[0.5], [0.5]])\n",
        "    cpd_G = TabularCPD(variable='G',\n",
        "                       variable_card=2,\n",
        "                       values=prob_G_given_A(disparity),\n",
        "                       evidence = [sensitive_attribute],\n",
        "                       evidence_card = [2]\n",
        "                       )\n",
        "    cpd_keep = TabularCPD(variable='keep',\n",
        "                       variable_card=2,\n",
        "                       values=prob_keep(disparity, discrimination),\n",
        "                       evidence = [sensitive_attribute, 'fair_Y'],\n",
        "                       evidence_card = [2,2]\n",
        "                       )\n",
        "    cpd_fair = TabularCPD(variable='fair_Y',\n",
        "                       variable_card=2,\n",
        "                       values= [[1-doubt,doubt],[doubt,1-doubt]],\n",
        "                       evidence = ['G'],\n",
        "                       evidence_card = [2]\n",
        "                       )\n",
        "    stat_parity_edges = ([tuple([sensitive_attribute, 'X' + str(i)]) for i in range (var_n)]+\n",
        "                         [tuple(['G', 'X' + str(i)]) for i in range (var_n)]+\n",
        "                         [(sensitive_attribute, label_names),(sensitive_attribute, 'G'), ('fair_Y', label_names)]+\n",
        "                         [('G','fair_Y')]+\n",
        "                         [(sensitive_attribute, 'keep'), ('fair_Y', 'keep') ])\n",
        "\n",
        "    stat_parity_bayes = BayesianNetwork(stat_parity_edges)\n",
        "    stat_parity_bayes.add_cpds(*cpd_X,cpd_Y,cpd_A,cpd_G,cpd_keep,cpd_fair)\n",
        "    stat_parity_bayes.check_model()\n",
        "    return BayesianModelSampling(stat_parity_bayes).forward_sample\n",
        "\n",
        "def generate_train(threshold, disparity):\n",
        "    stat_parity_bayes = generate_model(threshold, disparity)\n",
        "\n",
        "    df = stat_parity_bayes(size_train)\n",
        "    train_dataset_label = df.drop(columns = ['G', 'keep','fair_Y'])\n",
        "\n",
        "    new_size = int(size_train*size_train/ (df['keep'] == 1).sum())\n",
        "    train_dataset_selection = stat_parity_bayes(new_size)\n",
        "    filter = train_dataset_selection['keep'] == 1\n",
        "    train_dataset_selection = train_dataset_selection[filter].drop(columns = ['keep',label_names,'G'])\n",
        "    train_dataset_selection.rename(columns={'fair_Y': label_names},inplace=True)\n",
        "\n",
        "    test_dataset_fair = stat_parity_bayes(size_test)\n",
        "    test_dataset_fair.rename(columns={label_names:'unfair_Y', 'fair_Y': label_names},inplace=True)\n",
        "    test_dataset_fair = test_dataset_fair.drop(columns = ['keep','G','unfair_Y'])\n",
        "\n",
        "    return (train_dataset_label, train_dataset_selection, test_dataset_fair)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbKIoW6oU1lg"
      },
      "outputs": [],
      "source": [
        "def iter_model(iter_function, name, verbose = False, save = False):\n",
        "    for ind,discrimination in enumerate(discriminations):\n",
        "        avg_disparity_label = 0\n",
        "        avg_accuracy_label = 0\n",
        "        avg_disparity_selection = 0\n",
        "        avg_accuracy_selection = 0\n",
        "        for j in range(n_databases):\n",
        "            train_dataset_label, train_dataset_selection, test_dataset_fair = label_bias_datasets[ind][j], selection_bias_datasets[ind][j], fair_datasets[ind][j]\n",
        "            train_aif_label = BinaryLabelDataset(df=train_dataset_label, label_names = label_names, protected_attribute_names= sensitive_attribute)\n",
        "            train_aif_selection = BinaryLabelDataset(df=train_dataset_selection, label_names = label_names, protected_attribute_names= sensitive_attribute)\n",
        "            test_aif_fair = BinaryLabelDataset(df=test_dataset_fair, label_names = label_names, protected_attribute_names= sensitive_attribute)\n",
        "\n",
        "            if name+'_label_accuracy' not in risultati.columns:\n",
        "                risultati[name+'_label_accuracy'] = 0.0\n",
        "                risultati[name+'_label_disparity'] = 0.0\n",
        "                risultati[name+'_selection_accuracy'] = 0.0\n",
        "                risultati[name+'_selection_disparity'] = 0.0\n",
        "\n",
        "            mom_acc, mom_disp = iter_function(train_aif_label, test_aif_fair)\n",
        "            avg_disparity_label += mom_disp\n",
        "            avg_accuracy_label += mom_acc\n",
        "\n",
        "            mom_acc, mom_disp = iter_function(train_aif_selection, test_aif_fair)\n",
        "            avg_disparity_selection += mom_disp\n",
        "            avg_accuracy_selection += mom_acc\n",
        "            if verbose:\n",
        "                print('dataset done: disparity=',discrimination,'number=',j)\n",
        "        risultati.loc[discrimination][[name+'_label_accuracy', name+'_label_disparity']] = [avg_accuracy_label/n_databases, avg_disparity_label/n_databases]\n",
        "        risultati.loc[discrimination][[name+'_selection_accuracy', name+'_selection_disparity']] = [avg_accuracy_selection/n_databases, avg_disparity_selection/n_databases]\n",
        "        if save:\n",
        "            with open('/content/gdrive/My Drive/risultati.csv', 'w') as f:\n",
        "                f.write(risultati.to_csv())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maOkZhqsK-pz"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "label_bias_datasets = [[0 for j in range(n_databases)] for ind in discriminations]\n",
        "selection_bias_datasets = [[0 for j in range(n_databases)] for ind in discriminations]\n",
        "fair_datasets = [[0 for j in range(n_databases)] for ind in discriminations]\n",
        "for ind,discrimination in enumerate(discriminations):\n",
        "    for j in range(n_databases):\n",
        "        label_bias_datasets[ind][j], selection_bias_datasets[ind][j], fair_datasets[ind][j] = generate_train(threshold,disparity)\n",
        "\n",
        "risultati = pd.DataFrame(index = discriminations, dtype= 'float')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0hjCbR8SUpQ"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "def forest_result(aif_train, aif_test):\n",
        "\n",
        "    forest_classifier = RandomForestClassifier()\n",
        "    X_train = aif_train.features\n",
        "    y_train = aif_train.labels.ravel()\n",
        "\n",
        "    forest_classifier.fit(X_train, y_train)\n",
        "    predictions = forest_classifier.predict(aif_test.features)\n",
        "    return eval_model(predictions, aif_test.labels.ravel(), aif_test.protected_attributes.ravel() )\n",
        "\n",
        "iter_model(forest_result, 'forest')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFnMeykXUmdM"
      },
      "outputs": [],
      "source": [
        "with open('/content/gdrive/My Drive/risultati.csv', 'w') as f:\n",
        "  f.write(risultati.to_csv())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae3S4dmN3jxv"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "def reweighing_result(aif_train, aif_test):\n",
        "    '''Data Preprocessing Techniques for Classification without Discrimination'''\n",
        "    RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
        "                    privileged_groups=privileged_groups)\n",
        "    forest_classifier = RandomForestClassifier(random_state=42)\n",
        "    dataset_aif_reweight_train = RW.fit_transform(aif_train)\n",
        "    X_train = dataset_aif_reweight_train.features\n",
        "    y_train = dataset_aif_reweight_train.labels.ravel()\n",
        "\n",
        "    forest_classifier.fit(X_train, y_train, sample_weight=dataset_aif_reweight_train.instance_weights)\n",
        "    predictions = forest_classifier.predict(aif_test.features)\n",
        "    return eval_model(predictions, aif_test.labels.ravel(), aif_test.protected_attributes.ravel())\n",
        "\n",
        "iter_model(reweighing_result, 'reweighing')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5XYeF2-UpNt"
      },
      "outputs": [],
      "source": [
        "with open('/content/gdrive/My Drive/risultati.csv', 'w') as f:\n",
        "  f.write(risultati.to_csv())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7OLFMzON1cJ",
        "outputId": "8148c919-8d4d-4523-d6c7-0aefcb7a9155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1176: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ]
        }
      ],
      "source": [
        "%%capture\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "def adversarial_result(aif_train, aif_test):\n",
        "    sess = tf.Session()\n",
        "    adversarial_label_unfair = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
        "                            unprivileged_groups = unprivileged_groups,\n",
        "                            scope_name='adversarial_label_unfair',\n",
        "                            adversary_loss_weight=0.5,\n",
        "                            debias=True,\n",
        "                            sess=sess)\n",
        "\n",
        "    adversarial_label_unfair.fit(aif_train)\n",
        "    predictions = adversarial_label_unfair.predict(aif_test).labels.ravel()\n",
        "\n",
        "    sess.close()\n",
        "    tf.reset_default_graph()\n",
        "    return eval_model(predictions, aif_test.labels.ravel(), aif_test.protected_attributes.ravel())\n",
        "\n",
        "\n",
        "iter_model(adversarial_result, 'adversarial')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYqNrjWOUuUh"
      },
      "outputs": [],
      "source": [
        "with open('/content/gdrive/My Drive/risultati.csv', 'w') as f:\n",
        "  f.write(risultati.to_csv())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6iaOU9MYeYA"
      },
      "outputs": [],
      "source": [
        "from aif360.algorithms.preprocessing import DisparateImpactRemover\n",
        "\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "def DIR_result(aif_train, aif_test):\n",
        "    index = aif_train.feature_names.index(sensitive_attribute)\n",
        "\n",
        "    di = DisparateImpactRemover(repair_level=1)\n",
        "    train_repd = di.fit_transform(aif_train)\n",
        "    test_repd = di.fit_transform(aif_test)\n",
        "\n",
        "    X_tr = np.delete(train_repd.features, index, axis=1)\n",
        "    y_tr = train_repd.labels.ravel()\n",
        "    X_te = np.delete(test_repd.features, index, axis=1)\n",
        "\n",
        "    forest_classifier = RandomForestClassifier()\n",
        "\n",
        "    forest_classifier.fit(X_tr, y_tr)\n",
        "    predictions = forest_classifier.predict(X_te)\n",
        "    return eval_model(predictions, aif_test.labels.ravel(), aif_test.protected_attributes.ravel())\n",
        "\n",
        "iter_model(DIR_result, 'DIR')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmCgt5zpUvPC"
      },
      "outputs": [],
      "source": [
        "with open('/content/gdrive/My Drive/risultati.csv', 'w') as f:\n",
        "  f.write(risultati.to_csv())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "545GTIzDyfRx",
        "outputId": "e8724072-7ff9-40d3-88a4-af47a7aa7e1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset done: disparity= 0.0 number= 0\n",
            "dataset done: disparity= 0.0 number= 1\n",
            "dataset done: disparity= 0.0 number= 2\n",
            "dataset done: disparity= 0.0 number= 3\n",
            "dataset done: disparity= 0.0 number= 4\n",
            "dataset done: disparity= 0.0 number= 5\n",
            "dataset done: disparity= 0.0 number= 6\n",
            "dataset done: disparity= 0.0 number= 7\n",
            "dataset done: disparity= 0.0 number= 8\n",
            "dataset done: disparity= 0.0 number= 9\n",
            "dataset done: disparity= 0.1 number= 0\n",
            "dataset done: disparity= 0.1 number= 1\n",
            "dataset done: disparity= 0.1 number= 2\n",
            "dataset done: disparity= 0.1 number= 3\n",
            "dataset done: disparity= 0.1 number= 4\n",
            "dataset done: disparity= 0.1 number= 5\n",
            "dataset done: disparity= 0.1 number= 6\n",
            "dataset done: disparity= 0.1 number= 7\n",
            "dataset done: disparity= 0.1 number= 8\n",
            "dataset done: disparity= 0.1 number= 9\n",
            "dataset done: disparity= 0.2 number= 0\n",
            "dataset done: disparity= 0.2 number= 1\n",
            "dataset done: disparity= 0.2 number= 2\n",
            "dataset done: disparity= 0.2 number= 3\n",
            "dataset done: disparity= 0.2 number= 4\n",
            "dataset done: disparity= 0.2 number= 5\n",
            "dataset done: disparity= 0.2 number= 6\n",
            "dataset done: disparity= 0.2 number= 7\n",
            "dataset done: disparity= 0.2 number= 8\n",
            "dataset done: disparity= 0.2 number= 9\n",
            "dataset done: disparity= 0.30000000000000004 number= 0\n",
            "dataset done: disparity= 0.30000000000000004 number= 1\n",
            "dataset done: disparity= 0.30000000000000004 number= 2\n",
            "dataset done: disparity= 0.30000000000000004 number= 3\n",
            "dataset done: disparity= 0.30000000000000004 number= 4\n",
            "dataset done: disparity= 0.30000000000000004 number= 5\n",
            "dataset done: disparity= 0.30000000000000004 number= 6\n",
            "dataset done: disparity= 0.30000000000000004 number= 7\n",
            "dataset done: disparity= 0.30000000000000004 number= 8\n",
            "dataset done: disparity= 0.30000000000000004 number= 9\n",
            "dataset done: disparity= 0.4 number= 0\n",
            "dataset done: disparity= 0.4 number= 1\n",
            "dataset done: disparity= 0.4 number= 2\n",
            "dataset done: disparity= 0.4 number= 3\n",
            "dataset done: disparity= 0.4 number= 4\n",
            "dataset done: disparity= 0.4 number= 5\n",
            "dataset done: disparity= 0.4 number= 6\n",
            "dataset done: disparity= 0.4 number= 7\n",
            "dataset done: disparity= 0.4 number= 8\n",
            "dataset done: disparity= 0.4 number= 9\n",
            "dataset done: disparity= 0.5 number= 0\n",
            "dataset done: disparity= 0.5 number= 1\n",
            "dataset done: disparity= 0.5 number= 2\n",
            "dataset done: disparity= 0.5 number= 3\n",
            "dataset done: disparity= 0.5 number= 4\n",
            "dataset done: disparity= 0.5 number= 5\n",
            "dataset done: disparity= 0.5 number= 6\n",
            "dataset done: disparity= 0.5 number= 7\n",
            "dataset done: disparity= 0.5 number= 8\n",
            "dataset done: disparity= 0.5 number= 9\n",
            "dataset done: disparity= 0.6000000000000001 number= 0\n",
            "dataset done: disparity= 0.6000000000000001 number= 1\n",
            "dataset done: disparity= 0.6000000000000001 number= 2\n",
            "dataset done: disparity= 0.6000000000000001 number= 3\n",
            "dataset done: disparity= 0.6000000000000001 number= 4\n",
            "dataset done: disparity= 0.6000000000000001 number= 5\n",
            "dataset done: disparity= 0.6000000000000001 number= 6\n",
            "dataset done: disparity= 0.6000000000000001 number= 7\n",
            "dataset done: disparity= 0.6000000000000001 number= 8\n",
            "dataset done: disparity= 0.6000000000000001 number= 9\n",
            "dataset done: disparity= 0.7000000000000001 number= 0\n",
            "dataset done: disparity= 0.7000000000000001 number= 1\n",
            "dataset done: disparity= 0.7000000000000001 number= 2\n",
            "dataset done: disparity= 0.7000000000000001 number= 3\n",
            "dataset done: disparity= 0.7000000000000001 number= 4\n",
            "dataset done: disparity= 0.7000000000000001 number= 5\n",
            "dataset done: disparity= 0.7000000000000001 number= 6\n",
            "dataset done: disparity= 0.7000000000000001 number= 7\n",
            "dataset done: disparity= 0.7000000000000001 number= 8\n",
            "dataset done: disparity= 0.7000000000000001 number= 9\n",
            "dataset done: disparity= 0.8 number= 0\n",
            "dataset done: disparity= 0.8 number= 1\n",
            "dataset done: disparity= 0.8 number= 2\n",
            "dataset done: disparity= 0.8 number= 3\n",
            "dataset done: disparity= 0.8 number= 4\n",
            "dataset done: disparity= 0.8 number= 5\n",
            "dataset done: disparity= 0.8 number= 6\n",
            "dataset done: disparity= 0.8 number= 7\n",
            "dataset done: disparity= 0.8 number= 8\n",
            "dataset done: disparity= 0.8 number= 9\n",
            "dataset done: disparity= 0.9 number= 0\n",
            "dataset done: disparity= 0.9 number= 1\n",
            "dataset done: disparity= 0.9 number= 2\n",
            "dataset done: disparity= 0.9 number= 3\n",
            "dataset done: disparity= 0.9 number= 4\n",
            "dataset done: disparity= 0.9 number= 5\n",
            "dataset done: disparity= 0.9 number= 6\n",
            "dataset done: disparity= 0.9 number= 7\n",
            "dataset done: disparity= 0.9 number= 8\n",
            "dataset done: disparity= 0.9 number= 9\n"
          ]
        }
      ],
      "source": [
        "from aif360.algorithms.preprocessing.lfr import LFR\n",
        "from aif360.algorithms.preprocessing.lfr_helpers import helpers as lfr_helpers\n",
        "\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "def LFR_results(aif_train, aif_test):\n",
        "    scale_orig = StandardScaler()\n",
        "    LFR_train = aif_train.copy(deepcopy= True)\n",
        "    LFR_valid = aif_test.copy(deepcopy= True)\n",
        "\n",
        "    LFR_train.features = scale_orig.fit_transform(LFR_train.features)\n",
        "    LFR_valid.features = scale_orig.transform(LFR_valid.features)\n",
        "\n",
        "\n",
        "    TR = LFR(unprivileged_groups=unprivileged_groups,\n",
        "            privileged_groups=privileged_groups,\n",
        "            k=10, Ax=0.1, Ay=1.0, Az=5\n",
        "            )\n",
        "\n",
        "    R = TR.fit(LFR_train, maxiter=5000, maxfun=5000)\n",
        "    dataset_transf_valid = TR.transform(LFR_valid)\n",
        "    predictions = dataset_transf_valid.labels.ravel()\n",
        "    return eval_model(predictions, aif_test.labels.ravel(), aif_test.protected_attributes.ravel())\n",
        "\n",
        "iter_model(LFR_results, 'LFR', verbose = True, save = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4xDgc-w030p"
      },
      "outputs": [],
      "source": [
        "with open('/content/gdrive/My Drive/risultati.csv', 'w') as f:\n",
        "  f.write(risultati.to_csv())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwYnLHNjNpJVYwPFy9rH5R",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}